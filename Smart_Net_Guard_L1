# =============================================================================
# SmartNetGuard — Level 1 (L1) Detector: Deep Conv1D Autoencoder + optional Fine-Tune
# -----------------------------------------------------------------------------
# File purpose:
#   - Trains and stores an L1 model (deep 1D Conv autoencoder) for detection
#     anomalies on windows of fixed size (WINDOW_SIZE x num_features).
#   - Calculates reconstruction errors during validation and testing,
#     aggregates metrics (AR, trimmed AR, event-level metrics).
#   - Performs fine-tune on "difficult normals".
#   - Builds histograms, performs clustering of En4 embeddings (HDBSCAN/DBSCAN),
#     and saves auxiliary artifacts (scaler, preprocessing config for L2).
#
# Important terms and data forms:
#   - Initial data: two feature tables (train, test) row by row by time + binary labels for test.
#   - Standardization: StandardScaler (mean, std) → float32 array, z-clip up to ±8.
#   - Window: shape tensor (T, F) = (WINDOW_SIZE, num_features).
#   - Autoencoder: input/output shape (T, F); task: window reconstruction.
#   - Reconstruction error (per-window MSE): average MSE across time and features.
#   - AR (Attack/Normal ratio): mean(MSE|attack)/mean(MSE|normal) — the higher, the better the separability.
#   - Event-level metrics: event coverage with the top-K windows budget flag.
#   - En4: globally averaged hidden layer of the encoder - window embedding (used for clustering).
# =============================================================================

import json, random, gc, os

os.environ["KERAS_BACKEND"] = "tensorflow"
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA, IncrementalPCA
import tensorflow as tf
import keras
from keras.layers import (
    Input, Conv1D, BatchNormalization, LayerNormalization,
    LeakyReLU, Dropout, Add, Dense, RepeatVector,
    GlobalAveragePooling1D, Multiply
)
from keras.models import Model
from keras.optimizers import Adam
from keras.losses import Huber
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras import regularizers

# ============================ FILE PATHS ===================================
# Paths to data and output directory.
# TRAIN_PATH: parquet with training features (without labels).
# TEST_PATH : parquet with test features (without labels).
# LABEL_PATH: parquet with binary labels (0=normal, 1=attack) for testing.
TRAIN_PATH = r"C:\Users\farao\Desktop\Project Smart Net Guard\L1_dataset's\X_train_ready_full_cleaned_float32.parquet"
TEST_PATH = r"C:\Users\farao\Desktop\Project Smart Net Guard\L1_dataset's\X_test_ready_renamed_cleaned_float32.parquet"
LABEL_PATH = r"C:\Users\farao\Desktop\Project Smart Net Guard\L1_dataset's\y_test_binary_cleaned.parquet"

# The directory where runs (model artifacts/graphics/metrics) will be saved.
SAVE_DIR = r"C:\Users\farao\Desktop\Project Smart Net Guard\L1_ready_for_working"
os.makedirs(SAVE_DIR, exist_ok=True)

# ============================ HYPERPARAMETERS ==================================
# Basic hyperparameters for training and inference.
SEED = 42                 # we record all accidents for reproducibility
WINDOW_SIZE = 112         # length of the time window T (number of lines per window)
BATCH_SIZE = 256          # batch size when training an autoencoder
EPOCHS = 60               # number of autoencoder training epochs

# Window generator parameters for training:
CHUNK_WINDOWS = 100_000                             # target "pseudo-epoch" by windows
STEPS_PER_EPOCH = max(1, CHUNK_WINDOWS // BATCH_SIZE)  # gradient steps per epoch

# Validation/Final Test Window Proportion (in order)
VALIDATION_FRACTION = 0.8

# Learning stabilizers
USE_LAYER_NORM = True     # True → LayerNorm; False → BatchNorm
HUBER_DELTA = 0.1         # delta for Huber loss (robust regression MSE/MAE mix)
LEARNING_RATE = 1e-4      # Basic LR for the optimizer
WEIGHT_DECAY = 1e-4       # weight decay if AdamW is available
GRAD_CLIP_NORM = 1.0      # L2-norm gradient clipping

# === Fine-tune (additional training on “difficult normals”) ===
FINETUNE_ENABLED = True           # enable/disable FT procedure
FINETUNE_TOP_PCT_NORMALS = 0.10   # the top-% share of "difficult normals" by mistake, but in the safe zone
FINETUNE_MAX_WINDOWS = 120_000    # upper window boundary for FT
FINETUNE_SAFETY_MARGIN_WINDOWS = 12 # "coloring" around attacks to avoid them being captured in FT
FINETUNE_EPOCHS = 5               # FT eras
FINETUNE_LR_FACTOR = 0.60         # LR reduction for FT (LR * factor)
FINETUNE_BATCH_SIZE = 512         # batch for FT

# A set of budgets (top-K window shares) for which we calculate event-level metrics
EVENT_BUDGETS_TO_REPORT = [0.04]  # Example: 4% of top windows by mistake

# Fixing randomness for TF/NumPy/Random
tf.random.set_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)

# ============================ GPU MEMORY ======================================
# Soft tuning of GPU memory growth to avoid occupying the entire buffer in advance.
try:
    gpus = tf.config.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
except Exception:
    pass


# =============================================================================
# AUXILIARY LAYERS AND OPTIMIZER
# =============================================================================

def squeeze_and_excitation_vector(input_vec, ratio=8):
    """
    Squeeze-and-Excitation (SE) function module for vector representations.
    Purpose:
      - Re-weights embedding coordinates (channels) through a small "gating" network.
      - Helps the model focus on the most informative latent features.
    Entrance:
      input_vec: Tensor, shape (batch, C) — feature vector after bottleneck.
      ratio : the compression ratio of the hidden layer in SE (smaller → more compact).
    Exit:
      Tensor of the same shape (batch, C), scaled by the sigmoid from "gating".
    """
    num_units = int(input_vec.shape[-1])
    hidden_units = max(num_units // ratio, 4)
    gating = Dense(hidden_units, activation="relu")(input_vec)
    gating = Dense(num_units, activation="sigmoid")(gating)
    return Multiply()([input_vec, gating])


def choose_normalization(x, name=None):
    """
    Wrapper over the normalization choice:
      - LayerNorm: independent of batch size, stable during streaming.
      - BatchNorm: faster on large batches, but depends on batch statistics.
    """
    if USE_LAYER_NORM:
        return LayerNormalization(axis=-1, name=name)(x)
    else:
        return BatchNormalization(name=name)(x)


def build_optimizer_and_loss():
    """
    Designs an optimizer and loss function for an autoencoder.
    Logics:
      - If AdamW (Keras 3) is available, use it with weight decay.
      - Otherwise - fallback to Adam.
    Loss:
      - Huber(delta=HUBER_DELTA) is a robust alternative to MSE, more resistant to outliers.
    Returns:
      optimizer (tf.keras.optimizers.Optimizer),
      loss_fn   (tf.keras.losses.Loss),
      use_weight_decay (bool) — flag indicating whether AdamW was used.
    """
    use_weight_decay = False
    try:
        from keras.optimizers import AdamW  # Keras 3
        optimizer = AdamW(learning_rate=LEARNING_RATE,
                          weight_decay=WEIGHT_DECAY,
                          beta_1=0.9, beta_2=0.999,
                          amsgrad=True,
                          clipnorm=GRAD_CLIP_NORM)
        use_weight_decay = True
    except Exception:
        optimizer = Adam(learning_rate=LEARNING_RATE,
                         beta_1=0.9, beta_2=0.999,
                         amsgrad=True,
                         clipnorm=GRAD_CLIP_NORM)
        use_weight_decay = False

    loss_fn = Huber(delta=HUBER_DELTA)
    return optimizer, loss_fn, use_weight_decay


# =============================================================================
# MODEL BUILDING: DEEP 1D CONV. AUTOENCODER
# =============================================================================

def build_deep_conv1d_autoencoder(input_shape):
    """
    Builds a deep 1D Conv autoencoder for time window reconstruction.

    Arguments:
      input_shape: tuple (T, F)
        - T = WINDOW_SIZE (number of time steps in the window);
        - F = num_features (number of features per step).
    Model Input: Shape Tensor (None, T, F)
    Model output: Shape Tensor (None, T, F) - reconstruction of the original window.

    Architecture:
      Encoder:
        Conv1D(256,5)→Norm→LeakyReLU→Dropout
        Conv1D(128,5)→Norm→LeakyReLU→Dropout
        Conv1D( 64,3)→Norm→LeakyReLU→Dropout
        Conv1D( 32,3,name="En4_conv")→Norm→LeakyReLU→Dropout
        GlobalAveragePooling1D(name="En4") → window embedding (we will use it further in HDBSCAN)

      Bottleneck:
        Dense(8, name="Bottleneck_dense") → Norm → LeakyReLU → SE-vector (channel attenuation)

      Decoder:
        RepeatVector(T) → Conv blocks (32→64→128→256) with Norm/LeakyReLU/Dropout
        Skip link with En4_conv (Conv1D(32,1) + Add)
        Final projection Conv1D(F,1,name="Reconstruction").

    Compilation:
      - Optimizer: AdamW/Adam (build_optimizer_and_loss)
      - Loss: Huber (element-wise between input and output)
    """
    input_tensor = Input(shape=input_shape, name="Input")
    optimizer, loss_fn, use_weight_decay = build_optimizer_and_loss()
    kernel_reg = None if use_weight_decay else regularizers.l2(1e-5)

    # ENCODER
    En1 = Conv1D(256, 5, padding='same', kernel_regularizer=kernel_reg)(input_tensor)
    En1 = choose_normalization(En1)
    En1 = LeakyReLU()(En1)
    En1 = Dropout(0.35)(En1)

    En2 = Conv1D(128, 5, padding='same', kernel_regularizer=kernel_reg)(En1)
    En2 = choose_normalization(En2)
    En2 = LeakyReLU()(En2)
    En2 = Dropout(0.35)(En2)

    En3 = Conv1D(64, 3, padding='same', kernel_regularizer=kernel_reg)(En2)
    En3 = choose_normalization(En3)
    En3 = LeakyReLU()(En3)
    En3 = Dropout(0.30)(En3)

    En4_Conv = Conv1D(32, 3, padding='same', kernel_regularizer=kernel_reg, name="En4_conv")(En3)
    En4_Conv = choose_normalization(En4_Conv)
    En4_Conv = LeakyReLU()(En4_Conv)
    En4_Conv = Dropout(0.25)(En4_Conv)

    # Global time averaging yields window embedding ("En4")
    embedding_En4 = GlobalAveragePooling1D(name="En4")(En4_Conv)

    # BOTTLENECK
    Bn = Dense(8, name="Bottleneck_dense")(embedding_En4)
    Bn = choose_normalization(Bn)
    Bn = LeakyReLU()(Bn)

    # Squeeze-and-Excitation enhances the "useful" channels of the bottleneck vector
    bottleneck_refined = squeeze_and_excitation_vector(Bn, ratio=8)

    # DECODER
    # RepeatVector(T) "stretches" the vector backwards in time
    De1 = RepeatVector(input_shape[0], name="Repeat_T")(bottleneck_refined)
    De1 = Conv1D(32, 3, padding='same')(De1)
    De1 = choose_normalization(De1)
    De1 = LeakyReLU()(De1)
    De1 = Dropout(0.25)(De1)

    # Skip-link (shunt) with echo En4_Conv via 1x1 convolution
    skip_from_encoder = Conv1D(32, 1, padding='same')(En4_Conv)
    De1 = Add()([De1, skip_from_encoder])

    De2 = Conv1D(64, 3, padding='same')(De1)
    De2 = choose_normalization(De2)
    De2 = LeakyReLU()(De2)
    De2 = Dropout(0.30)(De2)

    De3 = Conv1D(128, 3, padding='same')(De2)
    De3 = choose_normalization(De3)
    De3 = LeakyReLU()(De3)
    De3 = Dropout(0.35)(De3)

    De4 = Conv1D(256, 3, padding='same')(De3)
    De4 = choose_normalization(De4)
    De4 = LeakyReLU()(De4)
    De4 = Dropout(0.35)(De4)

    # The final layer predicts F channels for each of the T timesteps.
    reconstruction_output = Conv1D(input_shape[1], 1, padding='same', name="Reconstruction")(De4)

    model = Model(input_tensor, reconstruction_output, name="SmartNetGuard_DeepConv1D_AE")
    model.compile(optimizer=optimizer, loss=loss_fn)
    return model


# =============================================================================
# STREAMING ERROR ASSESSMENT
# =============================================================================

def compute_reconstruction_errors_streaming(model, windows_tensor, batch_size=512):
    """
    Calculates reconstruction errors for multiple windows in batches (memory-efficient).

    Arguments:
      model          : trained autoencoder (input/output of form (T,F))
      windows_tensor : np.ndarray of shape (N, T, F) — a set of windows for evaluation
      batch_size     : batch size for prediction

    Returns:
      mean_error_per_window : np.ndarray of the form (N,T)
         average MSE over all (t,f) in each window (aggregated error for ranking)
      mean_error_per_feature : np.ndarray shape (N, F)
         average MSE over time for each feature (useful for feature diagnostics)
    """
    num_windows, T, num_features = windows_tensor.shape
    mean_error_per_window = np.empty((num_windows,), dtype=np.float32)
    mean_error_per_feature = np.empty((num_windows, num_features), dtype=np.float32)

    start = 0
    while start < num_windows:
        end = min(start + batch_size, num_windows)
        #ascontiguousarray: Speeds up copying in TF/NumPy
        batch_windows = np.ascontiguousarray(windows_tensor[start:end]).astype(np.float32)

        # Window reconstruction with an autoencoder
        batch_reconstructions = model.predict(
            batch_windows, batch_size=min(batch_size, len(batch_windows)), verbose=0
        ).astype(np.float32)

        # Squared error (element-wise), then averaging over time and features
        squared_errors = (batch_windows - batch_reconstructions) ** 2
        mean_error_per_window[start:end] = np.mean(squared_errors, axis=(1, 2)).astype(np.float32)
        mean_error_per_feature[start:end] = np.mean(squared_errors, axis=1).astype(np.float32)

        start = end
        del batch_windows, batch_reconstructions, squared_errors
        if start % (batch_size * 50) == 0:
            gc.collect()

    return mean_error_per_window, mean_error_per_feature


def compute_ar_value(mean_errors, labels_window_end):
    """
    Calculates AR = mean(MSE | attack) / mean(MSE | normal)
    - labels_window_end: Binary label of the window based on the label of the last point (0/1).
    - The higher the AR, the greater the reconstruction error of attacks relative to the norm.
    """
    normal_errors = mean_errors[labels_window_end == 0]
    attack_errors = mean_errors[labels_window_end == 1]
    return float(np.mean(attack_errors) / (np.mean(normal_errors) + 1e-12))


# =============================================================================
# WORKING WITH WINDOWS
# =============================================================================

def random_windows_generator_for_training(standardized_array, window_size, batch_size):
    """
    Infinite random window generator from an array of strings (T,F).
    Entrance:
      standardized_array: np.ndarray forms (N_rows, F) - already standardized features
      window_size      : window length T
      batch_size       : window batch size
    Exit (yield):
      (batch, batch) — input=output (autoencoder learns to reconstruct)
        batch has shape (batch_size, T, F)
    """
    total_rows = standardized_array.shape[0]
    while True:
        starts = np.random.randint(0, total_rows - window_size + 1, size=batch_size)
        batch = np.stack([standardized_array[i:i + window_size, :] for i in starts], axis=0).astype(np.float32)
        yield batch, batch


def make_tf_dataset(standardized_array, window_size, batch_size, steps_per_epoch):
    """
    Wrapper over random window generator → tf.data.Dataset with prefetch.
    Returns:
      dataset         : tf.data.Dataset (pairs (x,y) are the same; autoencoder)
      steps_per_epoch : just a proxy parameter to the top (not evaluated here)
    """
    output_signature = (
        tf.TensorSpec(shape=(None, window_size, standardized_array.shape[1]), dtype=tf.float32),
        tf.TensorSpec(shape=(None, window_size, standardized_array.shape[1]), dtype=tf.float32),
    )
    dataset = tf.data.Dataset.from_generator(
        lambda: random_windows_generator_for_training(standardized_array, window_size, batch_size),
        output_signature=output_signature
    ).prefetch(tf.data.AUTOTUNE)
    return dataset, steps_per_epoch


def build_sliding_windows(standardized_array, window_size):
    """
    Constructs sliding windows with stride=1 over the entire sequence (without shuffling).
    Entrance:
      standardized_array: np.ndarray (N_rows, F)
    Exit:
      windows: np.ndarray (N_rows - T + 1, T, F)
    """
    num_rows, num_features = standardized_array.shape
    windows = np.lib.stride_tricks.sliding_window_view(standardized_array, (window_size, num_features))[:, 0, :, :]
    return windows


def labels_any_in_window(labels_0_1, window_size):
    """
    Translate dot marks to binary window mark: 1 if at least one "1" was encountered in the window.
    Entrance:
      labels_0_1 : np.ndarray (N_rows,) — binary labels by rows
      window_size: the length of the window T
    Exit:
      np.ndarray (N_rows - T + 1) — 0/1 for each window according to the "any in window" rule
    """
    y = np.asarray(labels_0_1, dtype=np.int32).reshape(-1)
    cumulative = np.concatenate([[0], np.cumsum(y)])
    window_sums = cumulative[window_size:] - cumulative[:-window_size]
    return (window_sums > 0).astype(np.int32)


def trimmed_ar(mean_errors, labels_window_end, trim=0.01):
    """
    Trimmed AR is the same as AR, but the means are calculated after “truncation” by quantiles.
    Why: Robustness to extreme error outliers.
    """
    def tmean(arr):
        lo, hi = np.quantile(arr, trim), np.quantile(arr, 1 - trim)
        mask = (arr >= lo) & (arr <= hi)
        return float(np.mean(arr[mask]))

    normal = mean_errors[labels_window_end == 0]
    attack = mean_errors[labels_window_end == 1]
    return tmean(attack) / (tmean(normal) + 1e-12)


# =============================================================================
# POST-HOCK METRICS "BY EVENT"
# =============================================================================

def _binary_runs(bool_mask):
    """
    Splits a binary array into "races" of consecutive True values:
      [F F T T T F T] → [(2,4), (6,6)] (intervals inclusive).
    Used to group windows into events.
    """
    if len(bool_mask) == 0:
        return []
    arr = np.asarray(bool_mask, dtype=np.bool_)
    padded = np.pad(arr.astype(np.int8), (1, 1), constant_values=0)
    diffs = np.diff(padded)
    starts = np.where(diffs == 1)[0]
    ends = np.where(diffs == -1)[0] - 1
    return list(zip(starts, ends))


def _event_metrics_for_budget(any_in_window_groundtruth, scores, budget_share):
    """
    Calculates event metrics when the flag budget is limited:
      - We take the top-K windows by score (score = reconstruction error), K = ceil(budget_share * N).
      - We form the predicted binary mask (True for top-K).
      - Let's count:
        * event_recall: the proportion of GT events that have an overlap with the predicted windows
        * event_precision: the proportion of predicted "window clusters" that matched a given GT event
        * TTD (time-to-detect) — median/95th percentile of the shift of the first detection from the start of the event
    """
    N = len(scores)
    top_k = max(1, int(np.ceil(budget_share * N)))
    order_desc = np.argsort(scores)[::-1]
    flagged_idx = order_desc[:top_k]

    pred_mask = np.zeros(N, dtype=np.bool_)
    pred_mask[flagged_idx] = True

    gt_mask = (np.asarray(any_in_window_groundtruth) == 1)
    gt_runs = _binary_runs(gt_mask)
    pred_runs = _binary_runs(pred_mask)

    # Recall by events: how many GT events were attended to (at least one window was included)
    covered_events = 0
    for s, e in gt_runs:
        if pred_mask[s:e + 1].any():
            covered_events += 1
    event_recall = (covered_events / (len(gt_runs) + 1e-12)) if len(gt_runs) > 0 else 1.0

    # Event precision: what is the proportion of predicted "clusters" with intersection in GT
    matching_preds = 0
    for s, e in pred_runs:
        if gt_mask[s:e + 1].any():
            matching_preds += 1
    event_precision = (matching_preds / (len(pred_runs) + 1e-12)) if len(pred_runs) > 0 else 1.0

    # TTD metrics
    ttd_list = []
    for s, e in gt_runs:
        seg = pred_mask[s:e + 1]
        if seg.any():
            first = s + int(np.argmax(seg))
            ttd_list.append(first - s)
    ttd_median = float(np.median(ttd_list)) if len(ttd_list) else None
    ttd_p95 = float(np.percentile(ttd_list, 95)) if len(ttd_list) else None

    return {
        "budget": float(budget_share),
        "flagged_share": float(top_k / N),
        "gt_events": int(len(gt_runs)),
        "pred_events": int(len(pred_runs)),
        "event_recall": float(event_recall),
        "event_precision": float(event_precision),
        "ttd_median_windows": ttd_median,
        "ttd_p95_windows": ttd_p95,
    }


def compute_event_level_metrics(any_in_window_groundtruth, scores, budgets, split_name, run_dir):
    """
    Auxiliary wrapper: calculates event-level metrics for multiple budgets,
    Prints a summary report and saves it as JSON.
    """
    results = []
    print(f"\n Event-level metrics ({split_name}) — budgets: {', '.join([f'{b * 100:.1f}%' for b in budgets])}")
    for b in budgets:
        m = _event_metrics_for_budget(any_in_window_groundtruth, scores, b)
        results.append(m)
        print(f"  • budget={100 * m['budget']:.1f}% | flagged≈{100 * m['flagged_share']:.2f}% | "
              f"GT={m['gt_events']} | Pred={m['pred_events']} | "
              f"Recall@event={m['event_recall']:.3f} | Precision@event={m['event_precision']:.3f} | "
              f"TTD_med={m['ttd_median_windows']} | TTD_p95={m['ttd_p95_windows']}")
    out_path = os.path.join(run_dir, f"event_metrics_{split_name}.json")
    with open(out_path, "w") as f:
        json.dump({"split": split_name, "budgets": results}, f, indent=2)
    return results


# =============================================================================
# SERVICE FUNCTIONS FOR FINE-TUNE
# =============================================================================

def dilate_attack_mask(any_in_window_mask, margin):
    """
    Dilates the binary attack mask by ±margin windows.
    Why: When selecting "difficult normals", do not capture windows near attacks.
    """
    if margin <= 0:
        return any_in_window_mask.astype(bool)
    mask = any_in_window_mask.astype(bool).copy()
    idx = np.where(mask)[0]
    for i in idx:
        lo = max(0, i - margin)
        hi = min(len(mask) - 1, i + margin)
        mask[lo:hi + 1] = True
    return mask


def pick_hard_normal_windows_for_finetune(val_windows_tensor,
                                          y_any_in_window_val,
                                          val_mean_error_per_window,
                                          top_pct=0.10,
                                          max_windows=120_000,
                                          safety_margin_windows=8):
    """
    Selects "difficult normals" for FT:
      - We take windows marked as normal and located FAR from attacks (dilate margin).
      - Sort by reconstruction error (descending).
      - We take the top share top_pct, but not more than max_windows.
    Entrance:
      val_windows_tensor         : np.ndarray (N_val, T, F)
      y_any_in_window_val        : np.ndarray (N_val,) — 0/1 any-in-window markup
      val_mean_error_per_window  : np.ndarray (N_val,) — average error per window
    Exit:
      windows_for_ft: np.ndarray (K, T, F) or None if no safe windows were found.
    """
    assert len(val_windows_tensor) == len(y_any_in_window_val) == len(val_mean_error_per_window)

    normal_mask = (y_any_in_window_val == 0)
    dilated_attack_mask = dilate_attack_mask((y_any_in_window_val == 1), safety_margin_windows)
    safe_mask = normal_mask & (~dilated_attack_mask)

    candidate_idx = np.where(safe_mask)[0]
    if candidate_idx.size == 0:
        return None

    errors_candidates = val_mean_error_per_window[candidate_idx]
    order = np.argsort(errors_candidates)[::-1]
    top_k = max(1, int(np.ceil(top_pct * len(candidate_idx))))
    top_k = min(top_k, max_windows)

    chosen_idx = candidate_idx[order[:top_k]]
    windows_for_ft = val_windows_tensor[chosen_idx].astype(np.float32)
    return windows_for_ft


def make_supervised_ds_from_windows(windows_array, batch_size):
    """
    Generates a tf.data.Dataset from a fixed array of windows, shuffle+batch+prefetch.
    Used for FT: input=autoencoder output.
    """
    ds = tf.data.Dataset.from_tensor_slices((windows_array, windows_array))
    ds = ds.shuffle(min(10000, len(windows_array))).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds


# =============================================================================
# MAIN
# =============================================================================

def main():
    """
    The main scenario for training/evaluating/saving the L1 model.

    Input files:
      TRAIN_PATH: parquet with train features (N_train, F)
      TEST_PATH : parquet with test features (N_test, F)
      LABEL_PATH: parquet with binary labels for test (N_test,) - 0/1

    Main stages:
      1) Data loading; standardization (fit on train, transform on test) + z-clip.
      2) Training the autoencoder on random windows from train (stride=1 is not needed, we take random samples).
      3) Preparing sequential sliding windows for test, splitting into val/final.
      4) Calculation of reconstruction errors (val/final), AR metrics (normal/trimmed/any-in-window).
      5) Event-level metrics for a given flag budget.
      6) Fine-tune the "difficult normals" from the validation part of the test → re-evaluation.
      7) Clustering of En4 embeddings (HDBSCAN/DBSCAN) on the final subsample.
      8) Saving the model, standardizer, graphs, JSON reports, configuration for L2.
    """
    print("Loading data ...")
    training_dataframe = pd.read_parquet(TRAIN_PATH)
    test_dataframe = pd.read_parquet(TEST_PATH)
    test_labels_0_1 = pd.read_parquet(LABEL_PATH).values.squeeze().astype(np.int32)
    assert len(test_dataframe) == len(test_labels_0_1), "X_test and y_test must be the same length!"

    feature_names = list(training_dataframe.columns)
    num_features = training_dataframe.shape[1]
    print(f"Train rows: {len(training_dataframe):,}, Test rows: {len(test_dataframe):,}, Features: {num_features}")

    # ---------- Standardization + "Z-Crop" ----------
    # We fit StandardScaler to train (mean/std) and apply it to test.
    scaler = StandardScaler()
    train_standardized = scaler.fit_transform(training_dataframe.values).astype(np.float32)
    test_standardized = scaler.transform(test_dataframe.values).astype(np.float32)

    # Z-clip to ±Z_CLIP: Clips extreme z-values, increasing the robustness of the network.
    Z_CLIP = 8.0
    train_standardized = np.clip(train_standardized, -Z_CLIP, Z_CLIP).astype(np.float32)
    test_standardized = np.clip(test_standardized, -Z_CLIP, Z_CLIP).astype(np.float32)

    # ---------- Folder ----------
    # We create a unique launch folder and save the standardizer parameters (for L2 and reproducibility).
    run_dir = os.path.join(SAVE_DIR, f"run_{datetime.now().strftime('%d_%m_%Y_%H-%M-%S')}")
    os.makedirs(run_dir, exist_ok=True)
    np.save(os.path.join(run_dir, "std_mean.npy"), scaler.mean_.astype(np.float32))
    np.save(os.path.join(run_dir, "std_scale.npy"), scaler.scale_.astype(np.float32))
    print("Saved mean/std into run_dir.")

    # ---------- Model ----------
    # Build and compile the autoencoder. input_shape=(WINDOW_SIZE, num_features)
    model = build_deep_conv1d_autoencoder(input_shape=(WINDOW_SIZE, num_features))
    model.summary()

    # ---------- Splits ----------
    # We apply a holdout (5%) to the train to control overfitting during generative learning.
    total_rows_train = len(train_standardized)
    holdout_ratio = 0.05
    holdout_split_row = int((1.0 - holdout_ratio) * total_rows_train)

    train_for_fit = train_standardized[:holdout_split_row]
    train_holdout_norm = train_standardized[holdout_split_row:]

    # Datasets: infinite random windows are collected from train_for_fit and holdout.
    train_dataset, steps_per_epoch = make_tf_dataset(
        train_for_fit, WINDOW_SIZE, BATCH_SIZE, STEPS_PER_EPOCH
    )
    val_steps = max(1, (25_000 // BATCH_SIZE))  # small valid stream
    val_dataset, _ = make_tf_dataset(
        train_holdout_norm, WINDOW_SIZE, BATCH_SIZE, val_steps
    )

    # ---------- Callbacks ----------
    # EarlyStopping: stops training if val_loss does not improve (patience=6)
    # ReduceLROnPlateau: halves LR if plateau (patience=3)
    callbacks = [
        EarlyStopping(monitor="val_loss", patience=6, min_delta=1e-4, restore_best_weights=True),
        ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, verbose=1),
    ]

    print(f"Training: epochs={EPOCHS}, steps/epoch={steps_per_epoch}, val_steps={val_steps}, "
          f"batch={BATCH_SIZE}, window={WINDOW_SIZE}")

    # ---------- Training ----------
    # Training an autoencoder as an (x→x) reconstructor on randomly sampled windows.
    history = model.fit(
        train_dataset,
        epochs=EPOCHS,
        steps_per_epoch=steps_per_epoch,
        validation_data=val_dataset,
        validation_steps=val_steps,
        callbacks=callbacks,
        verbose=1,
    )

    # ---------- Curves ----------
    # We save the loss history and graph for reports.
    with open(os.path.join(run_dir, "train_history.json"), "w") as f:
        json.dump({k: [float(x) for x in v] for k, v in history.history.items()}, f, indent=2)

    plt.figure(figsize=(7, 4))
    plt.plot(history.history["loss"], label="loss")
    if "val_loss" in history.history:
        plt.plot(history.history["val_loss"], label="val_loss")
    plt.xlabel("epoch"); plt.ylabel("Huber loss")
    plt.grid(True, ls="--", alpha=0.5); plt.legend()
    plt.tight_layout(); plt.savefig(os.path.join(run_dir, "train_curve.png")); plt.show()

    # ---------- Save ----------
    # We save the autoencoder (Keras .keras) and run hyperparameters.
    model.save(os.path.join(run_dir, "SmartNetGuard_DeepConv1D_AE.keras"))
    with open(os.path.join(run_dir, "hyperparams.json"), "w") as f:
        json.dump({
            "WINDOW_SIZE": WINDOW_SIZE, "BATCH_SIZE": BATCH_SIZE, "EPOCHS": EPOCHS,
            "CHUNK_WINDOWS": CHUNK_WINDOWS, "STEPS_PER_EPOCH": steps_per_epoch,
            "USE_LAYER_NORM": USE_LAYER_NORM, "HUBER_DELTA": HUBER_DELTA,
            "LEARNING_RATE": LEARNING_RATE, "WEIGHT_DECAY": WEIGHT_DECAY,
            "GRAD_CLIP_NORM": GRAD_CLIP_NORM
        }, f, indent=4)

    # ======================== EVALUATION WINDOWS =====================
    # For the test, we prepare sequential windows stride=1 (without shuffling).
    print("Preparing validation & final test windows (stride=1, no shuffle) ...")
    test_windows_tensor = build_sliding_windows(test_standardized, WINDOW_SIZE)  # (Nw, T, F)

    # Window label based on the last timestep (more conservative than AR-type metrics)
    y_window_ends = test_labels_0_1[WINDOW_SIZE - 1:]
    num_windows_total = len(test_windows_tensor)

    # We divide into validation/final in order (simulating an online flow).
    split_index = int(VALIDATION_FRACTION * num_windows_total)

    # We also count "any-in-window" (window = 1 if at least one 1 was found in it)
    y_any_in_window_all = labels_any_in_window(test_labels_0_1, WINDOW_SIZE)
    y_any_in_window_val = y_any_in_window_all[:split_index]
    y_any_in_window_final = y_any_in_window_all[split_index:]

    val_windows_tensor = test_windows_tensor[:split_index]
    final_windows_tensor = test_windows_tensor[split_index:]
    y_window_end_val = y_window_ends[:split_index]
    y_window_end_final = y_window_ends[split_index:]

    print(f"val windows={len(val_windows_tensor):,} (pos={int((y_window_end_val == 1).sum())}, "
          f"neg={int((y_window_end_val == 0).sum())})")
    print(f"final windows={len(final_windows_tensor):,} (pos={int((y_window_end_final == 1).sum())}, "
          f"neg={int((y_window_end_final == 0).sum())})")

    # ========================= ERROR ASSESSMENT ======================
    # We calculate reconstruction errors on both splits
    print("Evaluating reconstruction errors (val) ...")
    val_mean_error_per_window, val_mean_error_per_feature = compute_reconstruction_errors_streaming(
        model, val_windows_tensor, batch_size=512
    )
    print("Evaluating reconstruction errors (final) ...")
    final_mean_error_per_window, final_mean_error_per_feature = compute_reconstruction_errors_streaming(
        model, final_windows_tensor, batch_size=512
    )

    # ============================== AR-METRICS ===============================
    # AR is the main indicator of attack/norm separability at the window level.
    ar_val = compute_ar_value(val_mean_error_per_window, y_window_end_val)
    ar_final = compute_ar_value(final_mean_error_per_window, y_window_end_final)
    print(f"AR-value (val):   {ar_val:.6f}")
    print(f" AR-value (final): {ar_final:.6f}")

    # Additionally, we calculate AR for “any-in-window” and trimmed AR (cut off the tails of 1%)
    ar_val_any = compute_ar_value(val_mean_error_per_window, y_any_in_window_val)
    ar_final_any = compute_ar_value(final_mean_error_per_window, y_any_in_window_final)
    ar_val_trim = trimmed_ar(val_mean_error_per_window, y_window_end_val, trim=0.01)
    ar_final_trim = trimmed_ar(final_mean_error_per_window, y_window_end_final, trim=0.01)
    print(f"AR(any-in-window)  val={ar_val_any:.4f} | final={ar_final_any:.4f}")
    print(f"AR(trimmed,1%)     val={ar_val_trim:.4f} | final={ar_final_trim:.4f}")

    # Saving numeric metrics in JSON for reports/CI
    with open(os.path.join(run_dir, "ar_values.json"), "w") as f:
        json.dump({"AR_val": float(ar_val), "AR_final": float(ar_final)}, f, indent=4)
    with open(os.path.join(run_dir, 'diagnostics.json'), 'w') as f:
        json.dump({
            "AR_val": float(ar_val), "AR_final": float(ar_final),
            "AR_val_any": float(ar_val_any), "AR_final_any": float(ar_final_any),
            "AR_val_trim01": float(ar_val_trim), "AR_final_trim01": float(ar_final_trim)
        }, f, indent=4)

    # ================== POST-HOCK EVENT METRICS =====================
    # We calculate the coverage of events with the budget flag top-K windows (on val/final).
    val_event_metrics = compute_event_level_metrics(
        y_any_in_window_val, val_mean_error_per_window, EVENT_BUDGETS_TO_REPORT, "val", run_dir)
    final_event_metrics = compute_event_level_metrics(
        y_any_in_window_final, final_mean_error_per_window, EVENT_BUDGETS_TO_REPORT, "final", run_dir)

    # ================== FINE-TUNE ==================
    # Additional training on "difficult normals" from the validation part (safe from attacks)
    if FINETUNE_ENABLED:
        print("\nFine-tuning on hard normals (validation split) ...")

        windows_for_ft = pick_hard_normal_windows_for_finetune(
            val_windows_tensor=val_windows_tensor,
            y_any_in_window_val=y_any_in_window_val,
            val_mean_error_per_window=val_mean_error_per_window,
            top_pct=FINETUNE_TOP_PCT_NORMALS,
            max_windows=FINETUNE_MAX_WINDOWS,
            safety_margin_windows=FINETUNE_SAFETY_MARGIN_WINDOWS
        )

        if windows_for_ft is None or len(windows_for_ft) < 10:
            print(" There are no safe windows for fine-tune - skip step.")
        else:
            print(f"   Windows selected for fine-tune: {len(windows_for_ft):,}")

            ds_ft = make_supervised_ds_from_windows(windows_for_ft, FINETUNE_BATCH_SIZE)

            # New optimizer/loss (same type), but we reduce LR by FINETUNE_LR_FACTOR times
            ft_optimizer, ft_loss_fn, _ = build_optimizer_and_loss()
            try:
                keras.backend.set_value(ft_optimizer.learning_rate, float(LEARNING_RATE * FINETUNE_LR_FACTOR))
            except Exception:
                pass
            model.compile(optimizer=ft_optimizer, loss=ft_loss_fn)

            ft_callbacks = [EarlyStopping(monitor="loss", patience=2, min_delta=1e-5, restore_best_weights=True)]

            history_ft = model.fit(
                ds_ft,
                epochs=FINETUNE_EPOCHS,
                verbose=1,
                callbacks=ft_callbacks,
            )

            # We save the retrained version separately
            finetune_dir = os.path.join(run_dir, "finetuned")
            os.makedirs(finetune_dir, exist_ok=True)
            model.save(os.path.join(finetune_dir, "SmartNetGuard_DeepConv1D_AE_finetuned.keras"))
            with open(os.path.join(finetune_dir, "finetune_info.json"), "w") as f:
                json.dump({
                    "windows_used": int(len(windows_for_ft)),
                    "top_pct_normals": float(FINETUNE_TOP_PCT_NORMALS),
                    "max_windows": int(FINETUNE_MAX_WINDOWS),
                    "safety_margin_windows": int(FINETUNE_SAFETY_MARGIN_WINDOWS),
                    "epochs": int(FINETUNE_EPOCHS),
                    "lr_factor": float(FINETUNE_LR_FACTOR),
                }, f, indent=2)

            # Revaluation after FT (on the same splits)
            print("\nRe-evaluating errors after fine-tune ...")
            val_mean_error_per_window_ft, val_mean_error_per_feature_ft = compute_reconstruction_errors_streaming(
                model, val_windows_tensor, batch_size=512
            )
            final_mean_error_per_window_ft, final_mean_error_per_feature_ft = compute_reconstruction_errors_streaming(
                model, final_windows_tensor, batch_size=512
            )

            ar_val_ft = compute_ar_value(val_mean_error_per_window_ft, y_window_end_val)
            ar_final_ft = compute_ar_value(final_mean_error_per_window_ft, y_window_end_final)
            print(f" AR after FT (val):   {ar_val_ft:.6f}")
            print(f" AR after FT (final): {ar_final_ft:.6f}")

            compute_event_level_metrics(
                y_any_in_window_val, val_mean_error_per_window_ft, EVENT_BUDGETS_TO_REPORT, "val_after_ft", finetune_dir
            )
            compute_event_level_metrics(
                y_any_in_window_final, final_mean_error_per_window_ft, EVENT_BUDGETS_TO_REPORT, "final_after_ft",
                finetune_dir
            )

            with open(os.path.join(finetune_dir, "compare_before_after.json"), "w") as f:
                json.dump({
                    "before": {"AR_val": float(ar_val), "AR_final": float(ar_final)},
                    "after": {"AR_val": float(ar_val_ft), "AR_final": float(ar_final_ft)}
                }, f, indent=2)

    # ============================== HISTOGRAMS ====================
    # Error distribution histograms (final/val) - for visual diagnostics
    plt.figure(figsize=(8, 4))
    plt.hist(final_mean_error_per_window, bins=100, alpha=0.8)
    plt.title("Reconstruction Error (final test)")
    plt.xlabel("MSE per window"); plt.ylabel("count")
    plt.grid(True, ls='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(run_dir, "hist_reconstruction_error_final.png"))
    plt.show()

    plt.figure(figsize=(8, 4))
    plt.hist(val_mean_error_per_window, bins=100, alpha=0.8)
    plt.title("Reconstruction Error (val)")
    plt.xlabel("MSE over window"); plt.ylabel("count")
    plt.grid(True, ls='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(run_dir, "hist_reconstruction_error_val.png"))
    plt.show()

    def save_per_feature_histograms(per_feature_errors, split_name):
        """
        Builds error histograms for each feature (averaged over time) — valid/final.
        Entrance:
          per_feature_errors: np.ndarray (N_windows, F)
          split_name : str ('val' or 'final') for file naming
        """
        num_features_local = per_feature_errors.shape[1]
        for feat_idx in range(num_features_local):
            plt.figure(figsize=(7, 3.5))
            plt.hist(per_feature_errors[:, feat_idx], bins=100, alpha=0.85)
            title = f"{feature_names[feat_idx] if feat_idx < len(feature_names) else f'Feature {feat_idx}'} error ({split_name})"
            plt.title(title)
            plt.xlabel("MSE over time (per window)"); plt.ylabel("count")
            plt.grid(True, ls='--', alpha=0.5)
            plt.tight_layout()
            plt.savefig(os.path.join(run_dir, f"hist_feature_{feat_idx}_{split_name}.png"))
            plt.show()

    save_per_feature_histograms(val_mean_error_per_feature, "val")
    save_per_feature_histograms(final_mean_error_per_feature, "final")

    # We save averages by feature (for quick text diagnostics)
    with open(os.path.join(run_dir, "per_feature_errors_val.json"), "w") as f:
        json.dump(dict(zip(feature_names, np.mean(val_mean_error_per_feature, axis=0).astype(float))), f, indent=4)
    with open(os.path.join(run_dir, "per_feature_errors_final.json"), "w") as f:
        json.dump(dict(zip(feature_names, np.mean(final_mean_error_per_feature, axis=0).astype(float))), f, indent=4)

    # ================= HDBSCAN/DBSCAN ON En4 ===================
    # We cluster the En4 embeddings of the final part windows to see if there are “clusters of similar anomalies.”
    print(" HDBSCAN on En4 embeddings (memory-safe mode) ...")
    try:
        import hdbscan
        hdbscan_available = True
        print("   hdbscan: available")
    except Exception as e:
        hdbscan_available = False
        print("   hdbscan: NOT available -> fallback to sklearn.DBSCAN")

    # We push the encoder "head" to the En4 layer (globally averaged embedding)
    embedding_model_En4 = Model(inputs=model.input, outputs=model.get_layer("En4").output)

    # Subsampling settings for clustering (memory/time saving)
    MAX_DBSCAN_POINTS = 80_000
    THIN_EVERY_K = 1
    PCA_COMPONENTS = 16
    BATCH_FOR_EMBEDDINGS = 1024

    num_windows_final = len(final_windows_tensor)
    indices_all = np.arange(0, num_windows_final, THIN_EVERY_K, dtype=np.int64)
    if len(indices_all) > MAX_DBSCAN_POINTS:
        indices_for_clustering = np.random.choice(indices_all, size=MAX_DBSCAN_POINTS, replace=False)
        indices_for_clustering.sort()
    else:
        indices_for_clustering = indices_all
    print(f"   using {len(indices_for_clustering):,} / {num_windows_final:,} windows for clustering")

    # We obtain En4 embeddings of the subsample
    en4_embeddings_subset = embedding_model_En4.predict(
        final_windows_tensor[indices_for_clustering], batch_size=BATCH_FOR_EMBEDDINGS, verbose=1
    ).astype(np.float32)
    print("   En4 shape:", en4_embeddings_subset.shape)

    # Reduce the dimensionality of IncrementalPCA embeddings (reliably from memory)
    ipca = IncrementalPCA(n_components=PCA_COMPONENTS, batch_size=8192)
    cursor = 0
    while cursor < en4_embeddings_subset.shape[0]:
        ipca.partial_fit(en4_embeddings_subset[cursor:cursor + 8192])
        cursor += 8192

    reduced_subset = np.zeros((en4_embeddings_subset.shape[0], PCA_COMPONENTS), dtype=np.float32)
    cursor = 0
    while cursor < en4_embeddings_subset.shape[0]:
        reduced_subset[cursor:cursor + 8192] = ipca.transform(en4_embeddings_subset[cursor:cursor + 8192]).astype(
            np.float32)
        cursor += 8192
    del en4_embeddings_subset; gc.collect()
    print("   reduced shape:", reduced_subset.shape)

    # Clustering: HDBSCAN (if available) or DBSCAN (fallback)
    if hdbscan_available:
        MIN_CLUSTER_SIZE = max(50, int(0.0015 * len(reduced_subset)))
        MIN_SAMPLES = int(0.5 * MIN_CLUSTER_SIZE)
        CLUSTER_EPSILON = 0.0
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=MIN_CLUSTER_SIZE,
            min_samples=MIN_SAMPLES,
            metric='euclidean',
            cluster_selection_epsilon=CLUSTER_EPSILON,
            core_dist_n_jobs=-1
        )
        cluster_labels = clusterer.fit_predict(reduced_subset)
        unique_labels, counts = np.unique(cluster_labels, return_counts=True)
        cluster_stats = {int(k): int(v) for k, v in zip(unique_labels, counts)}
        print(" HDBSCAN clusters (label: count) ->", cluster_stats)
        algo_name = "HDBSCAN"
    else:
        dbscan = DBSCAN(eps=0.8, min_samples=10)
        cluster_labels = dbscan.fit_predict(reduced_subset)
        unique_labels, counts = np.unique(cluster_labels, return_counts=True)
        cluster_stats = {int(k): int(v) for k, v in zip(unique_labels, counts)}
        print(" DBSCAN clusters (label: count) ->", cluster_stats)
        algo_name = "DBSCAN"

    # For interpretation, print the attack rate by cluster (if labels are available)
    try:
        y_final_subset = y_window_end_final[indices_for_clustering]
        print("\n Attack rate by clusters (on a subsample):")
        for k in sorted(cluster_stats.keys()):
            mask = (cluster_labels == k)
            if mask.any():
                attack_rate = float(np.mean(y_final_subset[mask] == 1))
                print(f"   cluster {k:>3}: size={mask.sum():>6} | attack_rate={attack_rate:.3f}")
    except Exception:
        pass

    # We save the clustering results and their 2D projection (PCA2) for the graph
    np.save(os.path.join(run_dir, "hdbscan_idx_subset.npy"), indices_for_clustering)
    np.save(os.path.join(run_dir, "hdbscan_labels_subset.npy"), cluster_labels)
    with open(os.path.join(run_dir, "hdbscan_cluster_stats.json"), "w") as f:
        json.dump({"subset_size": int(len(indices_for_clustering)),
                   "total_windows_final": int(num_windows_final),
                   "pca_components": int(PCA_COMPONENTS),
                   "clusters": cluster_stats,
                   "algo": algo_name}, f, indent=4)

    pca2 = PCA(n_components=2, random_state=SEED)
    reduced2d = pca2.fit_transform(reduced_subset)
    plt.figure(figsize=(9, 6))
    scatter = plt.scatter(reduced2d[:, 0], reduced2d[:, 1], c=cluster_labels, cmap='tab10', s=5, alpha=0.7)
    plt.colorbar(scatter, label=f"{algo_name} label")
    plt.title(f"{algo_name} on En4 (PCA 2D) — subset")
    plt.grid(True, ls='--', alpha=0.5)
    plt.tight_layout(); plt.savefig(os.path.join(run_dir, "hdbscan_pca_subset.png")); plt.show()
    del reduced_subset, reduced2d; gc.collect()

    # ---------- Preprocessing Configuration (for L2) ----------
    # Preprocessing config for L2 (so that L2 can properly standardize/clip L1 inputs):
    preprocessing_config = {
        "feature_names": feature_names,
        "standardizer": {
            "mean": scaler.mean_.astype(float).tolist(),
            "scale": scaler.scale_.astype(float).tolist()
        },
        "z_clip": [-float(Z_CLIP), float(Z_CLIP)],
        "window_size": int(WINDOW_SIZE),
        "error_metric": "MSE_window_mean",  # L2 can optionally use errors as an additional feature
        "threshold": {"type": "none"}       # We do not set thresholds at this level
    }
    with open(os.path.join(run_dir, "preprocessing_config.json"), "w") as f:
        json.dump(preprocessing_config, f, indent=4)

    # ---------- Final Brief Report ----------
    # A short summary of key metrics and clusters - in a text file.
    with open(os.path.join(run_dir, "summary.txt"), "w") as f:
        f.write(f"AR(val)={ar_val:.6f}\nAR(final)={ar_final:.6f}\n")
        f.write(f"AR_any(val)={ar_val_any:.6f}\nAR_any(final)={ar_final_any:.6f}\n")
        f.write(f"AR_trim01(val)={ar_val_trim:.6f}\nAR_trim01(final)={ar_final_trim:.6f}\n")
        f.write("Event-level metrics (val):\n")
        for m in val_event_metrics:
            f.write(f" budget={100 * m['budget']:.1f}% | flagged≈{100 * m['flagged_share']:.2f}% | "
                    f"GT={m['gt_events']} | Pred={m['pred_events']} | "
                    f"Recall@event={m['event_recall']:.3f} | Precision@event={m['event_precision']:.3f} | "
                    f"TTD_med={m['ttd_median_windows']} | TTD_p95={m['ttd_p95_windows']}\n")
        f.write("Event-level metrics (final):\n")
        for m in final_event_metrics:
            f.write(f" budget={100 * m['budget']:.1f}% | flagged≈{100 * m['flagged_share']:.2f}% | "
                    f"GT={m['gt_events']} | Pred={m['pred_events']} | "
                    f"Recall@event={m['event_recall']:.3f} | Precision@event={m['event_precision']:.3f} | "
                    f"TTD_med={m['ttd_median_windows']} | TTD_p95={m['ttd_p95_windows']}\n")
        f.write(f"{algo_name} clusters: {cluster_stats}\n")

    print("\n Done. Results in:", run_dir)
    print("   - model, std_mean.npy, std_scale.npy, preprocessing_config.json (threshold: none)")
    print("   - ar_values.json, diagnostics.json, event_metrics_val.json, event_metrics_final.json")
    print("   - histograms (*.png) for overall and per-feature errors (val & final)")
    print("   - hdbscan_idx_subset.npy, hdbscan_labels_subset.npy, hdbscan_cluster_stats.json")
    print("   - summary.txt — a short summary of key metrics")
    print("   - finetuned/ (if FT is enabled): model after FT and before/after comparison")


if __name__ == "__main__":
    main()
